<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <script src='https://kit.fontawesome.com/a076d05399.js'></script>
        <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.7/css/all.css">
        <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
        <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> -->
        <meta http-equiv="X-UA-Compatible" content="ie=edge">
        <!-- begin SEO -->
        <title>Evaluation Metrics - Forough Shirin Abkenar</title>
        <meta property="og:locale" content="en-US">
        <meta property="og:site_name" content="Evaluation Metrics">
        <meta property="og:title" content="Evaluation Metrics">
        <link rel="canonical" href="https://foroughshirinabkenar.github.io/mysite/genAI.html">
        <meta property="og:url" content="https://foroughshirinabkenar.github.io/mysite/genAI.html">
        <meta property="og:description" content="Evaluation Metrics">
        <script async="" src="//www.google-analytics.com/analytics.js"></script>
        <script type="application/ld+json"> { "@context" : "http://schema.org", "@type" : "Person", "name" : "Aritra Ghosh", "url" : "https://foroughshirinabkenar.github.io", "sameAs" : null } </script>
        <!-- end SEO -->
        <!-- http://t.co/dKP3o1e -->
        <meta name="HandheldFriendly" content="True">
        <meta name="MobileOptimized" content="320">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <script> document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js '; </script>
        <!-- For all browsers -->
        <link rel="stylesheet" href="https://foroughshirinabkenar.github.io/mysite/assets/css/main.css">
        <meta http-equiv="cleartype" content="on">
        <!-- start custom head snippets -->
        <meta name="theme-color" content="#ffffff">
        <link rel="stylesheet" href="https://foroughshirinabkenar.github.io/mysite/assets/css/academicons.css">
        <script type="text/x-mathjax-config;executed=true"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
        <script type="text/x-mathjax-config;executed=true"> MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } }); </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML" async=""></script>
        <!-- end custom head snippets -->
        <style id="fit-vids-style">.fluid-width-video-wrapper{width:100%;position:relative;padding:0;}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper object,.fluid-width-video-wrapper embed {position:absolute;top:0;left:0;width:100%;height:100%;}</style>
        <script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
            .MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
            .MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
            .MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
            .MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
        </style>
        <style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
                #MathJax_About.MathJax_MousePost {outline: none}
                .MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
                .MathJax_MenuItem {padding: 2px 2em; background: transparent}
                .MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
                .MathJax_MenuActive .MathJax_MenuArrow {color: white}
                .MathJax_MenuArrow.RTL {left: .5em; right: auto}
                .MathJax_MenuCheck {position: absolute; left: .7em}
                .MathJax_MenuCheck.RTL {right: .7em; left: auto}
                .MathJax_MenuRadioCheck {position: absolute; left: 1em}
                .MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
                .MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
                .MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
                .MathJax_MenuDisabled {color: GrayText}
                .MathJax_MenuActive {background-color: Highlight; color: HighlightText}
                .MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
                .MathJax_ContextMenu:focus {outline: none}
                .MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
                #MathJax_AboutClose {top: .2em; right: .2em}
                .MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
                .MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
                .MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
                .MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
                .MathJax_MenuClose:hover span {background-color: #CCC!important}
                .MathJax_MenuClose:hover:focus {outline: none}
        </style>
        <style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
        </style>
        <style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
            .MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
        </style>
        <style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
            #MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
            #MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
            #MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
        </style>
        <style type="text/css">.MathJax_Preview {color: #888}
            #MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
            #MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
            .MathJax_Error {color: #CC0000; font-style: italic}
        </style>
        <style type="text/css">.MJXp-script {font-size: .8em}
            .MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
            .MJXp-bold {font-weight: bold}
            .MJXp-italic {font-style: italic}
            .MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
            .MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
            .MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
            .MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
            .MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
            .MJXp-largeop {font-size: 150%}
            .MJXp-largeop.MJXp-int {vertical-align: -.2em}
            .MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
            .MJXp-display {display: block; text-align: center; margin: 1em 0}
            .MJXp-math span {display: inline-block}
            .MJXp-box {display: block!important; text-align: center}
            .MJXp-box:after {content: " "}
            .MJXp-rule {display: block!important; margin-top: .1em}
            .MJXp-char {display: block!important}
            .MJXp-mo {margin: 0 .15em}
            .MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
            .MJXp-denom {display: inline-table!important; width: 100%}
            .MJXp-denom > * {display: table-row!important}
            .MJXp-surd {vertical-align: top}
            .MJXp-surd > * {display: block!important}
            .MJXp-script-box > *  {display: table!important; height: 50%}
            .MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
            .MJXp-script-box > *:last-child > * {vertical-align: bottom}
            .MJXp-script-box > * > * > * {display: block!important}
            .MJXp-mphantom {visibility: hidden}
            .MJXp-munderover {display: inline-table!important}
            .MJXp-over {display: inline-block!important; text-align: center}
            .MJXp-over > * {display: block!important}
            .MJXp-munderover > * {display: table-row!important}
            .MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
            .MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
            .MJXp-mtr {display: table-row!important}
            .MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
            .MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
            .MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
            .MJXp-mlabeledtr {display: table-row!important}
            .MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
            .MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
            .MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
            .MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
            .MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
            .MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
            .MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
            .MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
            .MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
            .MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
            .MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
            .MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
            .MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
            .MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
        </style>
        <style>
          .table-wrap {overflow-x: auto;}
          #tab-eval-finetune {border-collapse: collapse; width: 100%; min-width: 720px; text-align: center;}
          #tab-eval-finetune caption {caption-side: top; font-weight: 600; margin-bottom: 0.5rem;}
          #tab-eval-finetune th,
          #tab-eval-finetune td {border: 1px solid #444; padding: 0.5rem 0.6rem; vertical-align: middle;}
          #tab-eval-finetune thead th {background: #f5f5f5;}
          /* Add cline effect under BLEU + ROUGE headers only */
          #tab-eval-finetune thead .cline-row th {border-top: none;}
          /* Draw a thick line under the second header row except the first column (PPL) */
          #tab-eval-finetune thead .cline-row th:not(:first-child) {border-bottom: 2px solid #000;}
        </style>
        <style>
          body {
            font-size: 16px;
            font-family: Arial, sans-serif;
          }
          h1 {
            font-size: 24;  /* adjust as you like */
            font-weight: bold;
            color: #333;
          }
          h2 {
            font-size: 22;  /* adjust as you like */
            font-weight: bold;
            color: #333;
          }
          h3 {
            font-size: 20px;  /* adjust as you like */
            font-weight: bold;
            color: #333;
          }
          h4 {
            font-size: 18px;  /* adjust as you like */
            font-weight: bold;
            color: #333;
          }
        </style>
    </head>

    <body data-new-gr-c-s-check-loaded="14.984.0" data-gr-ext-installed="">
        <div id="MathJax_Message" style="display: none;"></div>
        <!--[if lt IE 9]>
            <div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
        <![endif]-->
        <div class="masthead">
            <div class="masthead__inner-wrap">
                <div class="masthead__menu">
                    <nav id="site-nav" class="greedy-nav"> 
                        <button class="hidden" count="0"><div class="navicon"></div></button>
                        <ul class="visible-links">
                            <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://foroughshirinabkenar.github.io/mysite/index.html">Forough Shirin Abkenar</a></li>
                            <li class="masthead__menu-item"><a href="https://foroughshirinabkenar.github.io/mysite/publications.html">Publications</a></li>
                            <li class="masthead__menu-item"><a href="https://foroughshirinabkenar.github.io/mysite/bio.html">Bio</a></li>
                            <li class="masthead__menu-item"><a href="https://foroughshirinabkenar.github.io/mysite/traditional_ml.html">Traditional ML</a></li>
                            <li class="masthead__menu-item"><a href="https://foroughshirinabkenar.github.io/mysite/genAI.html">Gen AI Projects</a></li>
                        </ul>
                        <ul class="hidden-links hidden"></ul>
                    </nav>
                </div>
            </div>
        </div>
        <div id="main" role="main">
            <div class="sidebar sticky">
                <div itemscope="" itemtype="http://schema.org/Person">
                    <div class="author__avatar"> <img src="https://foroughshirinabkenar.github.io/mysite/images/profile.jpeg" class="author__avatar" alt="Forough Shirin Abkenar"></div>
                    <div class="author__content">
                        <h3 class="author__name">Forough Shirin Abkenar</h3>
                        <p class="author__bio">Ph.D., Electrical Engineering and Computer Science</p>
                    </div>
                    <div class="author__urls-wrapper">
                        <button class="btn btn--inverse">Follow</button>
                        <ul class="author__urls social-icons">
                            <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> California, USA</li>
                            <li><a href="mailto:fshirina@uci.edu" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i> Email</a></li>
                            <!--<li><a href="https://foroughshirinabkenar.wixsite.com/mysite" target="_blank"><i class="fab fa-wix" aria-hidden="true"></i> Wix</a></li>
                            <li><a href="https://iasl.ics.uci.edu/people/forough-shirin-abkenar/" target="_blank">UCI</a></li>
                            <li><a href="https://www.researchgate.net/profile/Forough_Shirin_Abkenar2" target="_blank"><i class="fab fa-researchgate" style='color:green' aria-hidden="true"></i> ResearchGate</a></li>
                            -->
                            <li><a href="https://www.linkedin.com/in/forough-s-1460b2198" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
                            <li><a href="https://github.com/foroughshirinabkenar" target="_blank"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
                            <li><a href="https://scholar.google.com/citations?user=TQ0vI44AAAAJ&hl=en" target="_blank"><i class="fas fa-graduation-cap"></i> Google Scholar</a></li>
                            <li><a href="https://ieeexplore.ieee.org/author/37086113585" target="_blank"><img src="https://foroughshirinabkenar.github.io/mysite/images/logos/IEEE.jpg" style="width: 3em;"> IEEE</a></li>
                            <li><a href="https://www.webofscience.com/wos/author/record/AAA-7697-2019" target="_blank"><img src="https://foroughshirinabkenar.github.io/mysite/images/logos/wos.png" style="width: 1em;"> Web of Science</a></li>
                        </ul>
                    </div>
                </div>
            </div>
            <article class="page" itemscope="" itemtype="http://schema.org/CreativeWork">
                <meta itemprop="headline" content="Evaluation Metrics">
                <meta itemprop="description" content="Evaluation Metrics">
                <div class="page__inner-wrap">
                    <header>
                        <h1 class="page__title" itemprop="headline">Evaluation Metrics</h1>
                        <p align="justify">
                          After training (or tuning/fitting) a classification model, it is important 
                          to evaluate its performance and assess how well it can make predictions. 
                          To do this, the model is provided with test data, i.e., unseen data that 
                          were not used during training, and its performance is measured using various 
                          evaluation metrics. Depending on the type of the ML model, various evaluation 
                          metrics are employed.
                        </p>

                        <h2>Classification</h2>
                        <p align="justify">
                            Several metrics can be used to evaluate the performance of a tuned model for 
                            classification purposes (binary classification). In this chapter, we focus on 
                            key metrics: <em>Accuracy</em>, <em>Recall</em> (or true positive rate), false 
                            positive rate, <em>Precision</em>, <em>F1-score</em>, and <em>ROC-AUC</em>.
                        </p>

                        <h3>Confusion Matrix</h3>
                          <p align="justify">
                              Before discussing evaluation metrics, it is important to understand the confusion 
                              matrix and its components, which play a crucial role in model evaluation [1].
                          </p>
                          <p align="justify">
                              A confusion matrix consists of four elements: true positive (TP), 
                              false positive (FP), true negative (TN), and false negative (FN). 
                              Figure I illustrates the confusion matrix, where columns represent 
                              the actual values and rows represent the predicted values. TP and TN occur 
                              when the model's predictions match the actual values. Conversely, 
                              if the model predicts a negative (positive) instance as positive (negative), 
                              it corresponds to FP (FN) [1].
                          </p>
                          <img src="images/traditional_ml/evaluation_metrics/confusion_matrix.png" alt="Confusion Matrix">
                          <p>Figure I: Confusion Matrix.</p>
                        
                          <h3>Accuracy</h3>
                          <p align="justify">
                              Accuracy indicates how accurate a model is to predict both positive and negative 
                              classes correctly [1].
                          </p>
                          <p>
                            \[
                              \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{FP} + \text{TN} + \text{FN}}
                            \]
                          </p>
                        
                          <h3>Recall</h3>
                          <p align="justify">
                              Recall, also known as true positive rate (TPR), measures how effectively a model 
                              identifies positive instances. Specifically, it is the proportion of true positive 
                              instances relative to all actual positive samples, including both correctly predicted 
                              positives and positive instances that were incorrectly predicted as negative [1].
                          </p>
                          <p>
                            \[
                              \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
                            \]
                          </p>
                        
                          <h3>False Positive Rate (FPR)</h3>
                          <p align="justify">
                              The false positive rate (FPR) measures how often a model incorrectly classifies 
                              negative instances as positive. Specifically, it is the proportion of negative 
                              samples that are misclassified as positive relative to all actual negative samples [1].
                          </p>
                          <p>
                            \[
                              \text{FPR} = \frac{\text{FP}}{\text{TN} + \text{FP}}
                            \]
                          </p>
                        
                          <h3>Precision</h3>
                          <p align="justify">
                              Precision describes how precise a model is to classify true positive instances. Precision 
                              measures the proportion of true positive instances among all predicted positive instances [1].
                          </p>
                          <p>
                            \[
                              \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
                            \]
                          </p>
                        
                          <h3>F1-score</h3>
                          <p align="justify">
                              In imbalanced datasets, the arithmetic mean can be skewed by high values. In contrast, 
                              the harmonic mean gives more weight to lower values. The F1-score 
                              (that is F\(_{\beta}\)-score with \(\beta = 1\)) is the harmonic mean of precision 
                              and recall, that guarantees a low value in either metric results in a correspondingly 
                              low F1-score [1]
                          </p>
                          <p>
                            \[
                              \text{F1-score} = \frac{(1 + \beta^2)\cdot \text{Precision}\cdot \text{Recall}}{\beta^2 \cdot \text{Precision} + \text{Recall}} \quad (\beta = 1)
                            \]
                          </p>
                        
                          <h3>ROC-AUC</h3>
                          <p align="justify">
                              Accuracy depends on a specific decision threshold. In contrast, the Receiver 
                              Operating Characteristic (ROC) curve provides a visual representation of 
                              model performance across all thresholds, making it threshold-independent. 
                              The ROC plots the True Positive Rate (TPR) against the False Positive Rate (FPR). 
                              The Area Under the Curve (AUC) quantifies the probability that the model will 
                              assign a higher score to a randomly chosen positive instance than to a randomly 
                              chosen negative one [1].
                          </p>
                          <p>
                              Figure II illustrates the ROC curve for a binary classification scenario in which 
                              the model makes predictions completely at random, like a coin flip. In this case, 
                              the AUC equals 0.5 [1].
                          </p>
                          <img src="images/traditional_ml/evaluation_metrics/roc_auc.png" alt="ROC-AUC">
                          <p>Figure II: ROC-AUC of completely random guesses.</p>
                        
                          <p align="justify">
                              The ROC-AUC concept can be extended to Precision-Recall (PR) curves to more 
                              effectively evaluate model performance. Figure III shows the PR curve and 
                              its corresponding AUC for an example scenario, providing insight into how 
                              the model balances precision and recall. PR-AUC is particularly useful for 
                              imbalanced datasets, where the positive class is rare, because it focuses 
                              on the model's performance with respect to the minority class rather than 
                              being dominated by the majority class, as can happen with ROC-AUC. In general, 
                              higher PR-AUC values indicate better model performance [1].
                          </p>
                          <img src="images/traditional_ml/evaluation_metrics/pr_auc.png" alt="PR-AUC">
                          <p>Figure III: PR-AUC example.</p>
                        
                          <h2>Regression</h2>
                          <p align="justify">
                              Unlike classification models, which predict discrete classes or labels, 
                              regression models produce continuous numerical values as output. Therefore, 
                              specific evaluation metrics are needed to assess the performance of regression 
                              models.
                          </p>
                        
                          <p align="justify">
                              In regression models, the key concept for evaluating performance is 
                              <em>error</em> (or <em>residual</em>). The error is defined as the difference 
                              between the actual value and the predicted value. Regression models are developed 
                              to minimize the error.
                          </p>
                          
                          <h3>Mean Absolute Error (MAE)</h3>
                          <p align="justify">
                              Mean absolute error (MAE) measures the arithmetic mean of absolute errors for 
                              predictions [2].
                          </p>
                        
                          <p>
                            \[
                              \text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |\text{Actual}_i - \text{Predicted}_i|
                            \]
                          </p>

                          <p align="justify">
                              MAE treats all errors equally, assigning the same weight to small and large errors. 
                              As a result, large errors are not penalized more heavily than small ones. This 
                              property also makes MAE relatively robust to outliers, i.e., extreme values that lie 
                              far from the typical range of the data. Overall, MAE is appropriate for balanced data [2].
                          </p>
                        
                          <h3>Mean Squared Error (MSE)</h3>
                          <p align="justify">
                              Mean squared error (MSE) calculates the arithmetic mean of squared differences between 
                              the actual data and the predicted values [2].
                          </p>
                        
                          <p>
                            \[
                              \text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (\text{Actual}_i - \text{Predicted}_i)^2
                            \]
                          </p>

                          <p align="justify">
                              Squaring the errors makes the MSE enable to penalize larger errors more heavily 
                              than smaller errors, making it sensitive to outliers in the data [2].
                          </p>
                        
                          <h3>Root Mean Squared Error (RMSE)</h3>
                          <p align="justify">
                              Root mean squared error (RMSE) measures to root of MSE, i.e., the arithmetic mean 
                              of squared differences between the actual data and the predicted values [2].
                          </p>
                        
                          <p>
                            \[
                              \text{RMSE} = \sqrt{\text{MSE}} = \sqrt{\frac{1}{N}\sum_{i=1}^{N} (\text{Actual}_i - \text{Predicted}_i)^2}
                            \]
                          </p>

                          <h3>R-squared Score</h3>
                          <p align="justify">
                              R-squared Score (R<sup>2</sup>-score) is a measure of goodness of fit, i.e., it measures how close 
                              the data points are to the fitted line [2].
                          </p>
                          
                          <p>
                            \[
                              R^2\text{-score} = 1 - \frac{\text{SS}_{\text{RES}}}{\text{SS}_{\text{TOT}}} = 1 - \frac{\sum_{i=1}^{N}(y_i - \hat{y}_i)^2}{\sum_{i=1}^{N}(y_i - \bar{y})^2},
                            \]
                          </p>

                          <p align="justify">
                              where SSR<sub>RES</sub> and SS<sub>TOT</sub> show the sum of squares residuals (errors) and total, 
                              respectively. The former, measures the sum of squared difference between predicted values and 
                              actual values. The latter calculated the sum of squared difference between the actual values 
                              and their arithmetic mean [2].
                          </p>
                        
                          <h2>Clustering</h2>
                          <p align="justify">
                              Unlike classification and regression problems, where data are labeled, clustering deals with 
                              unlabeled data. Since no ground-truth labels are available, clustering models group data based 
                              on similarities such as distance or distribution. As a result, traditional evaluation metrics 
                              used in classification or regression are not directly applicable to clustering.
                          </p>
                        
                          <h3>Silhouette Score</h3>
                          <p align="justify">
                              The Silhouette Score evaluates the quality of clustering by measuring how well each data 
                              point fits within its assigned cluster compared to neighboring clusters. A score close 
                              to +1 indicates that a data point is well-matched to its own cluster and far from other 
                              clusters, while a score near 0 suggests that the point lies on or near a cluster boundary. 
                              Negative scores (close to â€“1) indicate that a data point may have been misassigned to 
                              the wrong cluster. The overall clustering quality is typically assessed by the average 
                              silhouette score across all data points, with <b>higher values</b> reflecting better-defined 
                              and more clearly separated clusters [3].
                          </p>
                          
                          <p>
                            \[
                              s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}},
                            \]
                          </p>

                          <p align="justify">
                              where \(a(i)\) is the inter-cluster distance, i.e., average distance from point \(i\) to all 
                              other points in the same cluster; and \(b(i)\) represents the nearest-cluster distance, i.e., 
                              minimum average distance from point \(i\) to points in the nearest neighboring cluster [3].
                          </p>
                        
                          <h3>Davies-Bouldin Index</h3>
                          <p align="justify">
                              The Davies-Bouldin Index (DBI) is an internal evaluation metric that measures the quality of 
                              clustering based on the trade-off between cluster compactness and separation. Compactness 
                              is quantified by the average distance of data points within a cluster to their centroid, 
                              while separation is measured by the distance between cluster centroids. A <b>lower DBI</b> 
                              value indicates better clustering, as it reflects more compact clusters that are well separated 
                              from each other. Since DBI relies only on the data and cluster assignments, it does not require 
                              external ground-truth labels [3].
                          </p>
                          
                          <p>
                            \[
                              DBI = \frac{1}{k} \sum_{i=1}^{k} \max_{i \neq j} \left( \frac{S_i + S_j}{M_{ij}} \right),
                            \]
                          </p>

                          <p align="justify">
                              where \(k\) is the number of clusters; \(S_{i}\) represents the compactness of the cluster \(i\), 
                              that is, the average distance of all points of the cluster \(i\) to its centroid; and \(M_{i}\) 
                              indicates the separation between the clusters, that is, the distance between the centroids of 
                              the clusters $i$ and \(j\). Notably, \((S_{i} + S_{j}) / M_{ij}\) compares the similarity between 
                              clusters $i$ and \(j\) [3].
                          </p>
                          
                          <h2>Ranking and Recommendation</h2>
                          <p align="justify">
                              In ranking and recommendation tasks, the developed machine learning system generates a list of 
                              outputs ranked from highest to lowest relevance. However, according to the ground-truth data, 
                              some truly relevant items may be incorrectly classified as irrelevant. Evaluation metrics such 
                              as <b>Recall@k</b>, <b>Precision@k</b>, and <b>Mean Average Precision (mAP)</b> are commonly 
                              used to assess the model's performance in capturing and ranking the relevant results accurately.
                          </p>
                          
                          <h3>Precision@K</h3>
                          <p align="justify">
                              In ranking and recommendation tasks, the evaluation is typically limited to the top-K retrieved 
                              items. In this context, Precision@K measures how many of the items within the top-K positions 
                              are relevant. Formally, Precision@K is defined as the ratio of relevant items retrieved in the 
                              top-K results to K. Therefore, we have [4].
                          </p>
                          
                          <p>
                            \[
                              \text{Precision@K} = \frac{\text{TP}}{\text{TP} + \text{FP}}
                            \]
                          </p>

                          <p align="justify">
                              Figure IV illustrates an example of a ranking system with K $ = 5$. As shown, the model 
                              successfully identifies two relevant items out of four total relevant items within the 
                              top-K results. Therefore, the Precision@K value is computed as \(2 / 5 = 0.4\).
                          </p>
                          <img src="images/traditional_ml/evaluation_metrics/recall_precision_at_k.png" alt="Precision@K Example">
                          <p>Figure IV: An example for a single-list ranking system.</p>

                          <p align="justify">
                              It is worth noting that Precision@K is highly sensitive to the choice of K. Consider a 
                              case where only two relevant items exist in the ground-truth data, and the model 
                              successfully retrieves both within the top-K results. If K is set to a large value, 
                              the precision will decrease substantially, since the number of retrieved items increases 
                              while the number of relevant items remains constant.
                          </p>
                        
                          <h3>Recall@K</h3>
                          <p align="justify">
                              Recall is defined as the model's ability to identify true positive instances in its 
                              predictions. In ranking and recommendation tasks, Recall@K measures how many of the 
                              relevant items are correctly retrieved by the model within the top-K positions. 
                              Formally, Recall@K is defined as the ratio of relevant items retrieved in the top-K 
                              results to the total number of relevant items in the ground-truth data. Therefore, 
                              we have [4]
                          </p>
                          
                          <p>
                            \[
                              \text{Recall@K} = \frac{\text{TP}}{\text{TP} + \text{FN}}
                            \]
                          </p>

                          <p>
                              Referring to the example provided in Fig. IV, the Recall@K value is computed as 
                              \(2 / 4 = 0.5\). Therefore, Recall@K reduces the sensitivity to K observed in 
                              Precision@K. However, although Recall@K is less sensitive to the value of K 
                              compared to Precision@K, it is insensitive to the relative ranking of relevant 
                              items within the top-K positions. For example, consider a case where only two 
                              relevant items exist in the ground-truth data, and both are retrieved within the 
                              top-K results but appear at lower ranks (e.g., positions 9 and 10 when K \(= 10\)). 
                              In this case, Recall@K equals 1.0, even though the ranking quality is poor.
                          </p>
                        
                          <h3>Mean Reciprocal Rank@K (MRR@K)</h3>
                          <p align="justify">
                              To understand Mean Reciprocal Rank@K (MRR@K), it is necessary to introduce Reciprocal 
                              Rank@K (RR@K) first. RR@K is a metric that focuses on the position of the first relevant 
                              item within the top-K results. It is defined as [4].
                          </p>
                          
                          <p>
                            \[
                              \text{RR} = \frac{1}{\text{Rank of the first relevant item}}
                            \]
                          </p>

                          <p align="justify">
                              For example, considering the example in Fig. IV, the RR@K is given as \(1 / 2 = 0.5\). 
                              Considering \(U\) ranked lists in the results, each corresponding to a query in an 
                              information retrieval task or a user in a recommendation system, MRR@K is defined as 
                              the average of RR@K across all \(U\) lists, i.e.,
                          </p>

                          <p>
                            \[
                              \text{MRR@K} = \frac{1}{U} \sum_{u=1}^{U} RR@K_u
                            \]
                          </p>

                          <p  align="justify">
                             For instance, Fig. V illustrates a ranking system with three lists in the outputs. 
                              As shown, the positions of the first relevant items are 2, 3, and 2 in the first, 
                              the second, and the third lists, respectively. Therefore, the corresponding RR@K 
                              values are computed as \(1 / 2 = 0.5\), \(1 / 3  = 0.33\), and \(1 / 2 = 0.5\). 
                              Accordingly, the MRR@K is equal to \((0.5 + 0.33 + 0.5) / 3 = 0.44\). 
                          </p>
                          <img src="images/traditional_ml/evaluation_metrics/mrr_map_at_k.png" alt="MRR@K Example">
                          <p>Figure V: An example for a multi-list ranking system.</p>

                          <p  align="justify">
                             MRR@K addresses the limitation of Precision@K and Recall@K, both of which ignore 
                              the ranks of relevant items within the top-K results. However, MRR@K only accounts 
                              for the rank of the first relevant item and disregards the positions of other 
                              relevant items that may also appear within the top-K list.
                          </p>
                        
                          <h3>Mean Average Precision@K (mAP@K)</h3>
                          <p align="justify">
                              Unlike MMR@K that measures the rank of the first relevant item in the output lists, 
                              Mean Average Precision@K (mAP@K) considers all relevant elements in the top-K positions. 
                              mAP@K is the mean of AP@K values. Taking into account \(N_{u}\) relevant items in the 
                              list \(u\), we have [4]
                          </p>
                          
                          <p>
                            \[
                              \text{AP@K}_u = \frac{1}{N_u} \sum_{k=1}^{K} \text{Precision}(k) \cdot \text{rel}(k),
                            \]
                          </p>

                          <p align="justify">
                              where Precision\((k)\) is the precision of predictions at top-$k$ positions, where 
                              \(k = 1, \dots, K\); also, rel\((k)\) is the relevance score of the item, that can 
                              be a binary value (relevant/not relevant) or a graded score (e.g., 0, 1, 2, 3). In 
                              the case of AP@K, we defined is as a binary variable where is equal to 1 if the item 
                              is relevant, and 0 otherwise. For example, referring to Fig. V, for \(u = 1\), we have:
                          </p>

                          <ul>
                              <li>
                                  <p align="justify">
                                      \(k = 1\): Precision\((k) = 0 / 1 = 0\), rel\((k) = 0\)
                                  </p>
                              </li>
                              <li>
                                  <p align="justify">
                                      \(k = 2\): Precision\((k) = 1 / 2 = 0.5\), rel\((k) = 1\)
                                  </p>
                              </li>
                              <li>
                                  <p align="justify">
                                      \(k = 3\): Precision\((k) = 1 / 3 = 0.33\), rel\((k) = 0\)
                                  </p>
                              </li>
                              <li>
                                  <p align="justify">
                                      \(k = 4\): Precision\((k) = 2 / 4 = 0.5\), rel\((k) = 1\)
                                  </p>
                              </li>
                          </ul>

                          <p align="justify">
                              Therefore
                          </p>

                          <p>
                              \[
                                  \text{AP@K}_{1}= \frac{0\times 0 + 0.5\times 1 + 0.33\times 0 + 0.5\times 1}{2} = \frac{0 + 0.5 + 0 + 0.5}{2} = \frac{1.0}{2} = 0.5
                              \]
                          </p>

                          <p align="justify">
                              Accordingly, for \(u = 2\) and \(u = 3\), we have
                          </p>

                          <p>
                              \[
                                  \text{AP@K}_{2}= \frac{0\times 0 + 0\times 0 + 0.33\times 1 + 0.0.5\times 1}{2} = \frac{0 + 0 + 0.33 + 0.5}{2} = \frac{0.83}{2} = 0.42
                              \]
                          </p>

                          <p>
                              \[
                                  \text{AP@K}_{3}= \frac{0\times 0 + 0.5\times 1 + 0\times 0 + 0\times 0}{1} = \frac{0 + 0.5 + 0 + 0}{1} = \frac{0.5}{1} = 0.5
                              \]
                          </p>

                          <p align="justify">
                              mAP@K measure the mean of all AP@K values as
                          </p>

                          <p>
                            \[
                              \text{mAP@K} = \frac{1}{U} \sum_{u=1}^{U} \text{AP@K}_u
                            \]
                          </p>

                          <p align="justify">
                              Therefore, mAP@K value for the above-mentioned example in Fig. V is computed as
                          </p>

                          <p>
                            \[
                              \text{mAP@K}= \frac{0.5 + 0.42 + 0.5}{3} = \frac{1.42}{3} = 0.47
                            \]
                          </p>
                        
                          <h3>Normalized Discounted Cumulative Gain@K (nDCG@K)</h3>
                          <p align="justify">
                              It is important not only to determine whether an item is relevant within an output 
                              list but also to evaluate how relevant it is. None of the aforementioned metrics 
                              account for the degree of relevance of items. In contrast, Normalized Discounted 
                              Cumulative Gain@K (nDCG@K) captures this aspect by assigning graded relevance 
                              scores to items. To compute nDCG@K, the Discounted Cumulative Gain@K (DCG@K) is 
                              first calculated, which applies a logarithmic discount factor based on the position 
                              of each item in the ranked list, i.e., \(\log_{2}(i + 1)\). Formally, DCG@K is 
                              defined as [4]
                          </p>
                          
                          <p>
                            \[
                              \text{DCG@K} = \sum_{i=1}^{K} \frac{\text{rel}_i}{\log_2(i+1)}
                            \]
                          </p>

                          <p align="justify">
                              In addition to the predicted ranks, the Ideal DCG (IDCG) metric represents the maximum 
                              possible DCG for a given set of results. nDCG@K normalizes the DCG with respect to the IDCG, 
                              yielding a score between 0 and 1. Formally [4],
                          </p>

                          <p>
                            \[
                              \text{nDCG@K} = \frac{\text{DCG@K}}{\text{IDCG@K}}
                            \]
                          </p>

                          <p  align="justify">
                              For instance, consider the results in Fig. VI. There are three output lists, labeled 
                              1, 2, and 3, and an ideal list with the ideal ranking of the items.
                          </p>

                          <img src="images/traditional_ml/evaluation_metrics/ndcg_at_k.png" alt="nDCG@K Example">
                          <p>Figure VI: An example for nDCG.</p>

                          <p  align="justify">
                              Since the items are categorized as irrelevant, somewhat relevant, and relevant, 
                              we define relevance scores 0, 1, and 2, respectively. Therefore, for each 
                              list \(u\), the corresponding DCG@K\(_{u}\) is computed as
                          </p>

                          <p>
                              \[
                                  \text{DCG@K}_{1} = \frac{2}{\log_{2}(1+1)} + \frac{1}{\log_{2}(2+1)} + \frac{0}{\log_{2}(3+1)} + \frac{1}{\log_{2}(4+1)} = \frac{2}{1} + \frac{1}{1.59} + 0 + \frac{1}{2.32} = 2 + 0.63 + 0.43 = 3.06
                              \]
                          </p>

                          <p>
                              \[
                                  \text{DCG@K}_{2} = \frac{2}{\log_{2}(1+1)} + \frac{0}{\log_{2}(2+1)} + \frac{1}{\log_{2}(3+1)} + \frac{1}{\log_{2}(4+1)} = \frac{2}{1} + 0 + \frac{1}{2} + \frac{1}{2.32} = 2 + 0.5 + 0.43 = 2.93
                              \]
                          </p>
                        
                          <p>
                              \[
                                  \text{DCG@K}_{2} = \frac{1}{\log_{2}(1+1)} + \frac{0}{\log_{2}(2+1)} + \frac{2}{\log_{2}(3+1)} + \frac{1}{\log_{2}(4+1)} = \frac{1}{1} + 0 + \frac{2}{2} + \frac{1}{2.32} = 1 + 1 + 0.43 = 2.43
                              \]
                          </p>

                          <p  align="justify">
                              Also, IDCG@K value is given by
                          </p>

                          <p>
                              \[
                                  \text{IDCG@K} = \frac{2}{\log_{2}(1+1)} + \frac{1}{\log_{2}(2+1)} + \frac{1}{\log_{2}(3+1)} + \frac{0}{\log_{2}(4+1)} = \frac{2}{1} + \frac{1}{1.59} + \frac{1}{2} + 0 = 2 + 0.63 + 0.5 = 3.13
                              \]
                          </p>

                          <p  align="justify">
                              Accordingly, the nDCG@K for each list is calculated as
                          </p>

                          <p>
                              \[
                                  \text{nDCG@K}_{1} = \frac{\text{DCG@K}_{1}}{\text{IDCG@K}} = \frac{3.06}{3.13} = 0.98
                              \]
                          </p>

                          <p>
                              \[
                                  \text{nDCG@K}_{2} = \frac{\text{DCG@K}_{2}}{\text{IDCG@K}} = \frac{2.93}{3.13} = 0.94
                              \]
                          </p>

                          <p>
                              \[
                                  \text{nDCG@K}_{3} = \frac{\text{DCG@K}_{3}}{\text{IDCG@K}} = \frac{2.43}{3.13} = 0.78
                              \]
                          </p>
                      
                      <h2>References</h2>
                      <p align="justify">
                        [1] Google,
                        â€œMl concepts - crash course,â€
                        Google, accessed: 2025,
                        <a href="https://developers.google.com/machine-learning/crash-course/prereqs-and-prework" target="_blank">https://developers.google.com/machine-learning/crash-course/prereqs-and-prework</a>.
                      </p>
                      <p align="justify">
                        [2] E. Lewinson,
                        â€œA comprehensive overview of regression evaluation metrics,â€
                        Nvidia, accessed: April 20, 2023,
                        <a href="https://developer.nvidia.com/blog/a-comprehensive-overview-of-regression-evaluation-metrics/" target="_blank">
                          https://developer.nvidia.com/blog/a-comprehensive-overview-of-regression-evaluation-metrics/</a>.
                      </p>
                      <p align="justify">
                        [3] GeeksforGeeks,
                        â€œEvaluation metrics for search and recommendation systems,â€
                        GeeksforGeeks, accessed: July 23, 2025,
                        <a href="https://www.geeksforgeeks.org/machine-learning/clustering-metrics/" target="_blank">
                          https://www.geeksforgeeks.org/machine-learning/clustering-metrics/</a>.
                      </p>
                      <p align="justify">
                        [4] L. Monigatti,
                        â€œClustering metrics in machine learning,â€
                        Weaviate, accessed: May 28, 2024,
                        <a href="https://weaviate.io/blog/retrieval-evaluation-metrics" target="_blank">
                          https://weaviate.io/blog/retrieval-evaluation-metrics</a>.
                      </p>
                    </header>
                    <section class="page__content" itemprop="text">
                        <footer class="page__meta"></footer>
                    </section>
                </div>
            </article>
        </div>
        <div class="page__footer">
            <footer> 
                <!-- start custom footer snippets --> 
                <!-- <a href="/sitemap/">Sitemap</a> end custom footer snippets -->
                <!-- <div class="page__footer-follow">
                    <ul class="social-icons">
                        <li><strong>Follow:</strong></li> -->
                        <!-- <li><a href="http://github.com/arghosh"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li> -->
                        <!-- <li><a href="https://arghosh.github.io/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li> -->
                    <!-- </ul> -->
                <!-- </div> -->
                <!-- <div class="page__footer-copyright">Â© 2020 Forough Shirin Abkenar. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>. -->
                <!-- </div> -->
            </footer>
        </div>
        <script src="https://foroughshirinabkenar.github.io/mysite/assets/js/main.min.js"></script> 
        <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','//www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-113060154-1', 'auto'); ga('send', 'pageview'); </script>
    </body>

</html>
