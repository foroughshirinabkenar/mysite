<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <script src='https://kit.fontawesome.com/a076d05399.js'></script>
        <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.7/css/all.css">
        <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
        <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> -->
        <meta http-equiv="X-UA-Compatible" content="ie=edge">
        <!-- begin SEO -->
        <title>Gradient Boosting - Forough Shirin Abkenar</title>
        <meta property="og:locale" content="en-US">
        <meta property="og:site_name" content="Gradient Boosting">
        <meta property="og:title" content="Gradient Boosting">
        <link rel="canonical" href="https://foroughshirinabkenar.github.io/mysite/genAI.html">
        <meta property="og:url" content="https://foroughshirinabkenar.github.io/mysite/genAI.html">
        <meta property="og:description" content="Gradient Boosting">
        <script async="" src="//www.google-analytics.com/analytics.js"></script>
        <script type="application/ld+json"> { "@context" : "http://schema.org", "@type" : "Person", "name" : "Aritra Ghosh", "url" : "https://foroughshirinabkenar.github.io", "sameAs" : null } </script>
        <!-- end SEO -->
        <!-- http://t.co/dKP3o1e -->
        <meta name="HandheldFriendly" content="True">
        <meta name="MobileOptimized" content="320">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <script> document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js '; </script>
        <!-- For all browsers -->
        <link rel="stylesheet" href="https://foroughshirinabkenar.github.io/mysite/assets/css/main.css">
        <meta http-equiv="cleartype" content="on">
        <!-- start custom head snippets -->
        <meta name="theme-color" content="#ffffff">
        <link rel="stylesheet" href="https://foroughshirinabkenar.github.io/mysite/assets/css/academicons.css">
        <script type="text/x-mathjax-config;executed=true"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
        <script type="text/x-mathjax-config;executed=true"> MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } }); </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML" async=""></script>
        <!-- end custom head snippets -->
        <style id="fit-vids-style">.fluid-width-video-wrapper{width:100%;position:relative;padding:0;}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper object,.fluid-width-video-wrapper embed {position:absolute;top:0;left:0;width:100%;height:100%;}</style>
        <script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
            .MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
            .MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
            .MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
            .MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
        </style>
        <style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
                #MathJax_About.MathJax_MousePost {outline: none}
                .MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
                .MathJax_MenuItem {padding: 2px 2em; background: transparent}
                .MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
                .MathJax_MenuActive .MathJax_MenuArrow {color: white}
                .MathJax_MenuArrow.RTL {left: .5em; right: auto}
                .MathJax_MenuCheck {position: absolute; left: .7em}
                .MathJax_MenuCheck.RTL {right: .7em; left: auto}
                .MathJax_MenuRadioCheck {position: absolute; left: 1em}
                .MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
                .MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
                .MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
                .MathJax_MenuDisabled {color: GrayText}
                .MathJax_MenuActive {background-color: Highlight; color: HighlightText}
                .MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
                .MathJax_ContextMenu:focus {outline: none}
                .MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
                #MathJax_AboutClose {top: .2em; right: .2em}
                .MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
                .MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
                .MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
                .MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
                .MathJax_MenuClose:hover span {background-color: #CCC!important}
                .MathJax_MenuClose:hover:focus {outline: none}
        </style>
        <style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
        </style>
        <style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
            .MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
        </style>
        <style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
            #MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
            #MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
            #MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
        </style>
        <style type="text/css">.MathJax_Preview {color: #888}
            #MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
            #MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
            .MathJax_Error {color: #CC0000; font-style: italic}
        </style>
        <style type="text/css">.MJXp-script {font-size: .8em}
            .MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
            .MJXp-bold {font-weight: bold}
            .MJXp-italic {font-style: italic}
            .MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
            .MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
            .MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
            .MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
            .MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
            .MJXp-largeop {font-size: 150%}
            .MJXp-largeop.MJXp-int {vertical-align: -.2em}
            .MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
            .MJXp-display {display: block; text-align: center; margin: 1em 0}
            .MJXp-math span {display: inline-block}
            .MJXp-box {display: block!important; text-align: center}
            .MJXp-box:after {content: " "}
            .MJXp-rule {display: block!important; margin-top: .1em}
            .MJXp-char {display: block!important}
            .MJXp-mo {margin: 0 .15em}
            .MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
            .MJXp-denom {display: inline-table!important; width: 100%}
            .MJXp-denom > * {display: table-row!important}
            .MJXp-surd {vertical-align: top}
            .MJXp-surd > * {display: block!important}
            .MJXp-script-box > *  {display: table!important; height: 50%}
            .MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
            .MJXp-script-box > *:last-child > * {vertical-align: bottom}
            .MJXp-script-box > * > * > * {display: block!important}
            .MJXp-mphantom {visibility: hidden}
            .MJXp-munderover {display: inline-table!important}
            .MJXp-over {display: inline-block!important; text-align: center}
            .MJXp-over > * {display: block!important}
            .MJXp-munderover > * {display: table-row!important}
            .MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
            .MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
            .MJXp-mtr {display: table-row!important}
            .MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
            .MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
            .MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
            .MJXp-mlabeledtr {display: table-row!important}
            .MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
            .MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
            .MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
            .MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
            .MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
            .MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
            .MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
            .MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
            .MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
            .MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
            .MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
            .MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
            .MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
            .MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
        </style>
        <style>
          .table-wrap {overflow-x: auto;}
          #tab-eval-finetune {border-collapse: collapse; width: 100%; min-width: 720px; text-align: center;}
          #tab-eval-finetune caption {caption-side: top; font-weight: 600; margin-bottom: 0.5rem;}
          #tab-eval-finetune th,
          #tab-eval-finetune td {border: 1px solid #444; padding: 0.5rem 0.6rem; vertical-align: middle;}
          #tab-eval-finetune thead th {background: #f5f5f5;}
          /* Add cline effect under BLEU + ROUGE headers only */
          #tab-eval-finetune thead .cline-row th {border-top: none;}
          /* Draw a thick line under the second header row except the first column (PPL) */
          #tab-eval-finetune thead .cline-row th:not(:first-child) {border-bottom: 2px solid #000;}
        </style>
        <style>
          body {
            font-size: 16px;
            font-family: Arial, sans-serif;
          }
          h1 {
            font-size: 24;  /* adjust as you like */
            font-weight: bold;
            color: #333;
          }
          h2 {
            font-size: 22;  /* adjust as you like */
            font-weight: bold;
            color: #333;
          }
          h3 {
            font-size: 20px;  /* adjust as you like */
            font-weight: bold;
            color: #333;
          }
          h4 {
            font-size: 18px;  /* adjust as you like */
            font-weight: bold;
            color: #333;
          }
        </style>
    </head>

    <body data-new-gr-c-s-check-loaded="14.984.0" data-gr-ext-installed="">
        <div id="MathJax_Message" style="display: none;"></div>
        <!--[if lt IE 9]>
            <div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
        <![endif]-->
        <div class="masthead">
            <div class="masthead__inner-wrap">
                <div class="masthead__menu">
                    <nav id="site-nav" class="greedy-nav"> 
                        <button class="hidden" count="0"><div class="navicon"></div></button>
                        <ul class="visible-links">
                            <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://foroughshirinabkenar.github.io/mysite/index.html">Forough Shirin Abkenar</a></li>
                            <li class="masthead__menu-item"><a href="https://foroughshirinabkenar.github.io/mysite/publications.html">Publications</a></li>
                            <li class="masthead__menu-item"><a href="https://foroughshirinabkenar.github.io/mysite/bio.html">Bio</a></li>
                            <li class="masthead__menu-item"><a href="https://foroughshirinabkenar.github.io/mysite/traditional_ml.html">Traditional ML</a></li>
                            <li class="masthead__menu-item"><a href="https://foroughshirinabkenar.github.io/mysite/genAI.html">Gen AI Projects</a></li>
                        </ul>
                        <ul class="hidden-links hidden"></ul>
                    </nav>
                </div>
            </div>
        </div>
        <div id="main" role="main">
            <div class="sidebar sticky">
                <div itemscope="" itemtype="http://schema.org/Person">
                    <div class="author__avatar"> <img src="https://foroughshirinabkenar.github.io/mysite/images/profile.jpeg" class="author__avatar" alt="Forough Shirin Abkenar"></div>
                    <div class="author__content">
                        <h3 class="author__name">Forough Shirin Abkenar</h3>
                        <p class="author__bio">Ph.D., Electrical Engineering and Computer Science</p>
                    </div>
                    <div class="author__urls-wrapper">
                        <button class="btn btn--inverse">Follow</button>
                        <ul class="author__urls social-icons">
                            <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> California, USA</li>
                            <li><a href="mailto:fshirina@uci.edu" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i> Email</a></li>
                            <!--<li><a href="https://foroughshirinabkenar.wixsite.com/mysite" target="_blank"><i class="fab fa-wix" aria-hidden="true"></i> Wix</a></li>
                            <li><a href="https://iasl.ics.uci.edu/people/forough-shirin-abkenar/" target="_blank">UCI</a></li>
                            <li><a href="https://www.researchgate.net/profile/Forough_Shirin_Abkenar2" target="_blank"><i class="fab fa-researchgate" style='color:green' aria-hidden="true"></i> ResearchGate</a></li>
                            -->
                            <li><a href="https://www.linkedin.com/in/forough-s-1460b2198" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
                            <li><a href="https://github.com/foroughshirinabkenar" target="_blank"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
                            <li><a href="https://scholar.google.com/citations?user=TQ0vI44AAAAJ&hl=en" target="_blank"><i class="fas fa-graduation-cap"></i> Google Scholar</a></li>
                            <li><a href="https://ieeexplore.ieee.org/author/37086113585" target="_blank"><img src="https://foroughshirinabkenar.github.io/mysite/images/logos/IEEE.jpg" style="width: 3em;"> IEEE</a></li>
                            <li><a href="https://www.webofscience.com/wos/author/record/AAA-7697-2019" target="_blank"><img src="https://foroughshirinabkenar.github.io/mysite/images/logos/wos.png" style="width: 1em;"> Web of Science</a></li>
                        </ul>
                    </div>
                </div>
            </div>
            <article class="page" itemscope="" itemtype="http://schema.org/CreativeWork">
                <meta itemprop="headline" content="Gradient Boosting">
                <meta itemprop="description" content="Gradient Boosting">
                <div class="page__inner-wrap">
                    <header>
                        <h1 class="page__title" itemprop="headline">Gradient Boosting</h1>
                        <p align="justify">
                          Unlike Random Forest, which constructs independent trees on bootstrap samples 
                            to reduce variance and improve generalization, <b>Gradient Boosting</b> 
                            builds trees sequentially, where each new tree aims to correct the errors 
                            made by the previous ones to achieve higher predictive accuracy.
                        </p>

                        <p align="justify">
                            Gradient Boosting enhances the final prediction function, \(F(x)\), 
                            by combining \(M\) \textbf{weak learners}. In this approach, each subsequent 
                            weak learner is trained to minimize the residual errors (mistakes) of the 
                            preceding learners, and the aggregation of all weak learners produces a 
                            strong predictive model. Assuming \(f_{i}(x)\) represents the \(i\)-th weak 
                            learner, the overall model is expressed as [2]
                        </p>

                        <p>
                            \[
                                F(x) = \sum_{i=1}^{M} f_{i}(x)
                            \]
                        </p>

                        <p align="justify">
                            To implement this, the algorithm first initializes a loss function and an 
                            initial weak learner. The loss function must be <b>differentiable</b>, 
                            as Gradient Boosting is a <em>gradient-based</em> optimization procedure. 
                            The algorithm begins with an extremely weak learner, \(f_{1}(x)\) [2].
                        </p>

                        <p align="justify">
                            For each learner \(f_{j}(x)\), where \(j = 1, \ldots, M\), the algorithm 
                            computes the <b>negative gradient</b> of the loss function w.r.t. 
                            the predictions, i.e., [2]
                        </p>

                        <p>
                            \[
                                \hat{r}_{ji} = - \frac{\partial \mathcal{L}(y_{i}, \hat{y}_{i})}{\partial \hat{y}_{i}} \bigg|_{\hat{y}_{i} = F_{j}(x_{i})},
                            \]
                        </p>

                        <p  align="justify">
                            where \(y_{i}\) and \(\hat{y}_{i}\) are the actual and predicted values 
                            corresponding to the input \(x_{i}\), respectively [2].
                        </p>

                        <p  align="justify">
                            Next, the new weak learner, \(f_{j+1}(x)\), is trained on the residuals 
                            \(\hat{r}_{j}\) (i.e., the computed gradients) along with the original 
                            input data \(X\). The contribution of this new weak learner, denoted as 
                            \(\hat{\gamma}_{j+1}\), is obtained by solving [2]
                        </p>

                        <p>
                            \[
                                \hat{\gamma}_{j + 1} = \argmin_{\gamma} \sum_{i=1}^{N} \mathcal{L} \left( y_{i}, F_{j}(x_{i}) + \gamma f_{j+1}(x_{i}) \right)
                            \]
                        </p>

                        <p  align="justify">
                            The model is then updated as [2]
                        </p>

                        <p>
                            \[
                                F_{j + 1}(x) = F_{j}(x) + \hat{\gamma}_{j + 1} f_{j + 1}(x)
                            \]
                        </p>

                        <p  align="justify">
                            This process continues iteratively until the algorithm converges 
                            to the optimal solution (i.e., the minimum loss) [2].
                        </p>

                        <p  align="justify">
                            <b>Comparison with Random Forest:</b>

                            <ul>
                                <li>
                                    <p  align="justify">
                                        Since each weak learner in Gradient Boosting is trained to 
                                        correct the errors of its predecessors, it often achieves 
                                        <b>higher predictive accuracy</b> than Random Forest.
                                    </p>
                                </li>
                                <li>
                                    <p  align="justify">
                                        However, Gradient Boosting is <b>slower to train</b> and 
                                        <b>more complex to tune</b>, as it involves more 
                                        hyperparameters.
                                    </p>
                                </li>
                                <li>
                                    <p  align="justify">
                                        It is also <b>more susceptible to overfitting</b> if not 
                                        properly regularized (e.g., via learning rate, subsampling, 
                                        or tree depth constraints).
                                    </p>
                                </li>
                            </ul>
                        </p>

                        
                         <h2>Implementation</h2>
                          <p>
                            The machine learning random fores model in Python was developed from scratch. 
                              The complete implementation script is available on 
                            <a href="https://github.com/foroughshirinabkenar/traditional_ml_tutorial/blob/main/gradient_boosting_scratch.ipynb" target="_blank">
                              Gradient Boosting from Scratch</a>.
                          </p>

                          <h3>Classifier</h3>
                          <p align="justify">
                              We developed a gradient boosting classifier for the <b>Breast Cancer</b> dataset 
                              using the built-in functions provided in <em>scikit-learn</em> [1] in Python. 
                              The following screenshot illustrates the training process of the 
                              classifier. The model was imported from 
                              sklearn.<span style="color: blue;">ensemble</span>.  
                              Moreover, we applied <b>Grid Search Cross-Validation</b> to identify 
                              the optimal hyperparameters of the model. In this process, we defined ranges 
                              for various various parameters, such as <em>n_estimators</em> (no. of trees), 
                              <em>max_depth</em>, <em>min_samples_split</em>, and 
                              <em>learning_rate</em>.

                              <center>
                                  <img src="https://foroughshirinabkenar.github.io/mysite/images/traditional_ml/supervised_learning/gb_clf_train.png">
                            </center>
                          </p>
                        
                          <p align="justify">
                            After identifying the best classifier through Grid Search Cross-Validation, 
                              we evaluated its performance using the test dataset. The following screenshot 
                              presents the corresponding results.
                            The complete implementation script is available on the GitHub page 
                            <a href="https://github.com/foroughshirinabkenar/traditional_ml_tutorial/blob/main/gradient_boosting_classification.ipynb" target="_blank">
                              Gradient Boosting Classifier</a>.

                            <center>
                              <img src="https://foroughshirinabkenar.github.io/mysite/images/traditional_ml/supervised_learning/gb_clf_test.png">
                            </center>
                          </p>

                        <h3>Regressor</h3>
                          <p align="justify">
                              We developed a gradient boosting regressor using the <b>California Housing</b> dataset 
                              available in scikit-learn. The following screenshot shows the training 
                              (fitting) process of the regressor, where <b>Grid Search Cross-Validation</b> 
                              was employed to tune the optimal hyperparameters.

                              <center>
                                  <img src="https://foroughshirinabkenar.github.io/mysite/images/traditional_ml/supervised_learning/gb_reg_train.png">
                            </center>
                          </p>

                        <p align="justify">
                            After training the regressor, we evaluated its performance on the test dataset. 
                            The following screenshot displays the corresponding results. The complete 
                            implementation script is available on the GitHub page 
                            <a href="https://github.com/foroughshirinabkenar/traditional_ml_tutorial/blob/main/gradient_boosting_regression.ipynb" target="_blank">
                              Gradient Boosting Regressor</a>.
                            
                            <center>
                              <img src="https://foroughshirinabkenar.github.io/mysite/images/traditional_ml/supervised_learning/gb_reg_test.png">
                            </center>
                          </p>

                     <h2>XGBoost</h2>
                        <p align="justify">
                            Extreme Gradient Boosting (XGBoost) is an enhanced variation of the traditional 
                            Gradient Boosting algorithm that provides significant improvements in speed, 
                            accuracy, and feature handling. These enhancements are achieved through advanced 
                            techniques such as regularization, parallel processing, and a more sophisticated 
                            loss function optimization using the second-order Taylor approximation instead of 
                            standard first-order gradient descent. Key characteristics of XGBoost include:

                            <ul>
                                <li>
                                    <p align="justify">
                                        XGBoost incorporates both L1-norm and L2-norm regularization techniques 
                                        to prevent overfitting and improve model generalization.
                                    </p>
                                </li>
                                <li>
                                    <p align="justify">
                                        It includes built-in mechanisms for handling missing values efficiently, 
                                        eliminating the need for external preprocessing.
                                    </p>
                                </li>
                                <li>
                                    <p align="justify">
                                        XGBoost inherently supports cross-validation, which helps in reducing 
                                        overfitting and improving model robustness.
                                    </p>
                                </li>
                                <li>
                                    <p align="justify">
                                        It is computationally efficient and supports parallel processing via 
                                        leveraging multi-core architectures for faster training and tuning.
                                    </p>
                                </li>
                                <li>
                                    <p align="justify">
                                        XGBoost employs a depth-first tree growth strategy and stops branching 
                                        when further splits provide no performance gain. The model then performs 
                                        backward pruning to optimize the final tree structure.
                                    </p>
                                </li>
                                <li>
                                    <p align="justify">
                                        It supports user-defined objective functions and evaluation metrics that 
                                        enhances versatility and provides flexibility for a wide range of learning 
                                        tasks.
                                    </p>
                                </li>
                            </ul>
                        </p>

                        <p align="justify">
                            Compared to traditional Gradient Boosting, XGBoost leverages a 
                            <b>Taylor series approximation</b> of the loss function to accelerate convergence. 
                            In XGBoost, the overall objective function is defined as
                        </p>

                        <p>
                            \[
                                \mathcal{L}(\Phi) = \sum_{i}\mathcal{L}(y_{i}, \hat{y}_{i}) + \sum_{k}\Omega(f_{k}),
                            \]
                        </p>

                        <p align="justify">
                            where the first term represents the residual loss w.r.t. the samples, aiming to minimize 
                            the bias, and the second term denotes the regularization component over all trees, aiming 
                            to control model complexity and reduce variance.
                        </p>

                        <p align="justify">
                            The residual loss at iteration $t$ combines the predictions of all previous trees with 
                            the contribution from the current tree, such that
                        </p>

                        <p>
                            \[
                                \mathcal{L}(y_{i}, \hat{y}_{i}) = \mathcal{L}(y_{i}, \hat{y}_{i(t-1)} + f_{t}(x_{i})),
                            \]
                        </p>

                        <p align="justify">
                            where \(\hat{y}_{i(t-1)}\) represents the aggregated predictions from the previous 
                            \((t-1)\) trees, and \(f_{t}(x_{i})\) corresponds to the prediction of the current tree.
                        </p>

                        <p align="justify">
                            To efficiently optimize this objective, XGBoost applies a second-order Taylor series 
                            approximation to the loss function around \(\hat{y}_{i(t-1)}\), that yields in
                        </p>

                        <p>
                            \[
                                \mathcal{L}(y_{i}, \hat{y}_{i(t-1)} + f_{t}(x_{i})) \approx \mathcal{L}(y_{i}, \hat{y}_{i}) + g_{i}f_{t}(x_{i}) + \frac{1}{2}h_{i}f_{t}^{2}(x_{i}),
                            \]
                        </p>

                        <p align="justify">
                            where \(g_{i}\) and \(h_{i}\) denote the first- and second-order derivatives 
                            (gradient and Hessian) of the loss function w.r.t. the previous prediction, defined as
                            \(g_{i} = \partial_{\hat{y}_{i(t-1)}}\mathcal{L}(y_{i}, \hat{y}_{i(t-1)})\) and 
                            \)h_{i} = \partial^{2}_{\hat{y}_{i(t-1)}}\mathcal{L}(y_{i}, \hat{y}_{i(t-1)})\).
                        </p>

                        <h3>Classifier</h3>
                            <p align="justify">
                                  We developed an XGBoost classifier for the <b>Breast Cancer</b> dataset 
                                  using the built-in functions provided in <em>scikit-learn</em> [1] in Python. 
                                  The following screenshot illustrates the training process of the 
                                  classifier. The model was imported from 
                                  <em><b>xgboost</b></em>.  
                                  Moreover, we applied <b>Grid Search Cross-Validation</b> to identify 
                                  the optimal hyperparameters of the model. In this process, we defined ranges 
                                  for various various parameters, such as <em>n_estimators</em> (no. of trees), 
                                  <em>max_depth</em>, <em>min_samples_split</em>, and 
                                  <em>learning_rate</em>.
    
                                  <center>
                                      <img src="https://foroughshirinabkenar.github.io/mysite/images/traditional_ml/supervised_learning/xgb_clf_train.png">
                                </center>
                              </p>

                            <p align="justify">
                            After identifying the best classifier through Grid Search Cross-Validation, 
                              we evaluated its performance using the test dataset. The following screenshot 
                              presents the corresponding results.
                            The complete implementation script is available on the GitHub page 
                            <a href="https://github.com/foroughshirinabkenar/traditional_ml_tutorial/blob/main/xgboost_classification.ipynb" target="_blank">
                              XGBoost Classifier</a>.

                            <center>
                              <img src="https://foroughshirinabkenar.github.io/mysite/images/traditional_ml/supervised_learning/xgb_clf_test.png">
                            </center>
                          </p>

                        <h3>Regressor</h3>
                          <p align="justify">
                              We developed an XGBoost regressor using the <b>California Housing</b> dataset 
                              available in scikit-learn. The following screenshot shows the training 
                              (fitting) process of the regressor, where <b>Grid Search Cross-Validation</b> 
                              was employed to tune the optimal hyperparameters.

                              <center>
                                  <img src="https://foroughshirinabkenar.github.io/mysite/images/traditional_ml/supervised_learning/xgb_reg_train.png">
                            </center>
                          </p>

                        <p align="justify">
                            After training the regressor, we evaluated its performance on the test dataset. 
                            The following screenshot displays the corresponding results. The complete 
                            implementation script is available on the GitHub page 
                            <a href="https://github.com/foroughshirinabkenar/traditional_ml_tutorial/blob/main/xgboost_regression.ipynb" target="_blank">
                              XGBoost Regressor</a>.
                            
                            <center>
                              <img src="https://foroughshirinabkenar.github.io/mysite/images/traditional_ml/supervised_learning/xgb_reg_test.png">
                            </center>
                          </p>
                      
                      <h2>References</h2>
                      <p align="justify">
                        [1] F.Pedregosa, G.Varoquaux,A.Gramfort, V.Michel, B.Thirion, O.Grisel,M.Blondel, 
                        P.Prettenhofer, R.Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, 
                        M. Brucher, M. Perrot, and E. Duchesnay, “Scikit-learn: Machine learning in Python,” 
                        Journal of Machine Learning Research, vol. 12, pp. 2825–2830, 2011.
                      </p>
                      <p align="justify">
                        [2] ritvikmath,
                        “Gradient Boosting : Data Science's Silver Bullet,”
                        ritvikmath, accessed: September 29, 2021,
                        <a href="https://www.youtube.com/watch?v=en2bmeB4QUo&t=608s" target="_blank">
                          https://www.youtube.com/watch?v=en2bmeB4QUo&t=608s</a>.
                      </p>
                    </header>
                    <section class="page__content" itemprop="text">
                        <footer class="page__meta"></footer>
                    </section>
                </div>
            </article>
        </div>
        <div class="page__footer">
            <footer> 
                <!-- start custom footer snippets --> 
                <!-- <a href="/sitemap/">Sitemap</a> end custom footer snippets -->
                <!-- <div class="page__footer-follow">
                    <ul class="social-icons">
                        <li><strong>Follow:</strong></li> -->
                        <!-- <li><a href="http://github.com/arghosh"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li> -->
                        <!-- <li><a href="https://arghosh.github.io/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li> -->
                    <!-- </ul> -->
                <!-- </div> -->
                <!-- <div class="page__footer-copyright">© 2020 Forough Shirin Abkenar. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>. -->
                <!-- </div> -->
            </footer>
        </div>
        <script src="https://foroughshirinabkenar.github.io/mysite/assets/js/main.min.js"></script> 
        <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','//www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-113060154-1', 'auto'); ga('send', 'pageview'); </script>
    </body>

</html>
